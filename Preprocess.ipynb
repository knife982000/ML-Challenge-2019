{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Carga de datos\n",
    "Los datos de entrenamiento se cargan con Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title label_quality  \\\n",
      "0  Hidrolavadora Lavor One 120 Bar 1700w  Bomba A...    unreliable   \n",
      "1                  Placa De Sonido - Behringer Umc22    unreliable   \n",
      "2               Maquina De Lavar Electrolux 12 Kilos    unreliable   \n",
      "3  Par Disco De Freio Diant Vent Gol 8v 08/ Frema...    unreliable   \n",
      "4  Flashes Led Pestañas Luminoso Falso Pestañas P...    unreliable   \n",
      "\n",
      "     language                   category  \n",
      "0     spanish  ELECTRIC_PRESSURE_WASHERS  \n",
      "1     spanish                SOUND_CARDS  \n",
      "2  portuguese           WASHING_MACHINES  \n",
      "3  portuguese        VEHICLE_BRAKE_DISCS  \n",
      "4     spanish            FALSE_EYELASHES  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('rslp')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('omw') #WORDNET\n",
    "#nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "'''import pyspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "number_cores = 8\n",
    "memory_gb = 10\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setMaster('local[{}]'.format(number_cores))\n",
    "        .set('spark.driver.memory', '{}g'.format(memory_gb))\n",
    ")\n",
    "sc = SparkContext(appName=\"Meli\", conf=conf)\n",
    "'''\n",
    "\n",
    "ds = pd.read_csv('../MLChallenge/data/train.csv')\n",
    "print(ds.head())\n",
    "ds_test = pd.read_csv('../MLChallenge/data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fanat\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'def count_words(x):\\n    def merge(x, y):\\n        res = {}\\n        for k in (x.keys() - y.keys()):\\n            res[k] = x[k]\\n        for k in (y.keys() - x.keys()):\\n            res[k] = y[k]\\n        for k in (x.keys() & y.keys()):\\n            res[k] = x[k] + y[k]\\n        return res\\n    return reduce(merge,map(lambda x:{w:1 for w in set(x)}, x))\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import re\n",
    "from gensim.utils import deaccent\n",
    "from collections import Counter\n",
    "\n",
    "trans = {ord(c):' ' for c in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'}\n",
    "ref = re.compile(\"\\d+[\\.,]\\d+\")\n",
    "red = re.compile(\"\\d+\")\n",
    "rem = re.compile(\".*[A-Z]+.*\")\n",
    "s_stem = nltk.stem.SnowballStemmer('spanish').stem\n",
    "#p_stem = nltk.stem.RSLPStemmer().stem\n",
    "p_stem = nltk.stem.SnowballStemmer('portuguese').stem\n",
    "\n",
    "s_sw = set(nltk.corpus.stopwords.words('spanish'))\n",
    "p_sw = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def proc_text(text, stem, sw):\n",
    "    text = text.lower()\n",
    "    text = ref.sub('FLOAT', text)\n",
    "    text = red.sub('INT', text)\n",
    "    text = text.translate(trans)\n",
    "    text = text.split()\n",
    "    #text = word_tokenize(text)\n",
    "    text = [stem(w) if rem.match(w) is None else w for w in text if w not in sw]\n",
    "    text = ' '.join(text)\n",
    "    if len(text)==0:\n",
    "        text = 'ART'\n",
    "    return deaccent(text)\n",
    "\n",
    "def process_words(x):\n",
    "    if x[2] == 'spanish':\n",
    "        stem = s_stem\n",
    "        sw = s_sw\n",
    "    else:\n",
    "        stem = p_stem\n",
    "        sw = p_sw\n",
    "    text = proc_text(x[0], stem, sw)\n",
    "    return [text]\n",
    "\n",
    "def process_words_test(x):\n",
    "    if x[2] == 'spanish':\n",
    "        stem = s_stem\n",
    "        sw = s_sw\n",
    "    else:\n",
    "        stem = p_stem\n",
    "        sw = p_sw\n",
    "    text = proc_text(x[1], stem, sw)\n",
    "    return [text]\n",
    "\n",
    "\n",
    "def count_words(x):\n",
    "    res = Counter()\n",
    "    for s in tqdm(x):\n",
    "        s = set(s)\n",
    "        for i in s:\n",
    "            res[i] = res[i] + 1\n",
    "    return res\n",
    "\n",
    "\n",
    "'''def count_words(x):\n",
    "    def merge(x, y):\n",
    "        res = {}\n",
    "        for k in (x.keys() - y.keys()):\n",
    "            res[k] = x[k]\n",
    "        for k in (y.keys() - x.keys()):\n",
    "            res[k] = y[k]\n",
    "        for k in (x.keys() & y.keys()):\n",
    "            res[k] = x[k] + y[k]\n",
    "        return res\n",
    "    return reduce(merge,map(lambda x:{w:1 for w in set(x)}, x))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidrolav lavor one INT bar INTw bomb alumini itali']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_words(ds.values[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 20000000/20000000 [00:30<00:00, 659773.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 246955/246955 [00:00<00:00, 478705.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os.path\n",
    "\n",
    "\n",
    "if os.path.exists('post_train.csv'):\n",
    "    print('Loading train...')\n",
    "    ds_train = pd.read_csv('post_train.csv')\n",
    "    post_proc = ds_train.values.tolist()\n",
    "else:\n",
    "    post_proc = [process_words(x) for x in tqdm(ds.values)]\n",
    "    ds_train = pd.DataFrame(data=np.asarray(post_proc), columns=['text'])\n",
    "    ds_train.to_csv('post_train.csv', index=False)\n",
    "\n",
    "del ds_train\n",
    "#Por cuestiones de memoria lo hago feo, se puede hacer inline\n",
    "#pero requiere duplicar estructuras\n",
    "for i in tqdm(range(len(post_proc))):\n",
    "    post_proc[i] = str(post_proc[i][0]).split()\n",
    "\n",
    "if os.path.exists('post_test.csv'):\n",
    "    print('Loading test...')\n",
    "    ds_test_a = pd.read_csv('post_test.csv')\n",
    "    post_proc_test = ds_test_a.values.tolist()\n",
    "else:\n",
    "    post_proc_test = [process_words_test(x) for x in tqdm(ds_test.values)]\n",
    "    ds_test_a = pd.DataFrame(data=np.asarray(post_proc_test), columns=['text'])\n",
    "    ds_test_a.to_csv('post_test.csv', index=False)\n",
    "    \n",
    "del ds_test_a\n",
    "#Por cuestiones de memoria lo hago feo\n",
    "for i in tqdm(range(len(post_proc_test))):\n",
    "    post_proc_test[i] = str(post_proc_test[i][0]).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando datasets SP-PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_train(ds, x):\n",
    "    count = ds['language'].value_counts()\n",
    "    x_new_sp = [None] * count['spanish']\n",
    "    quality_sp = [None] * count['spanish']\n",
    "    y_sp = [None] * count['spanish']\n",
    "    i_sp = 0\n",
    "    \n",
    "    x_new_pt = [None] * count['portuguese']\n",
    "    quality_pt = [None] * count['portuguese']\n",
    "    y_pt = [None] * count['portuguese']\n",
    "    i_pt = 0\n",
    "    \n",
    "    for row, s in tqdm(zip(ds.values, x), total=len(x)):\n",
    "        if row[2] == 'spanish':\n",
    "            x_new_sp[i_sp] = s\n",
    "            quality_sp[i_sp] = row[1]\n",
    "            y_sp[i_sp] = row[3]\n",
    "            i_sp += 1\n",
    "        else:\n",
    "            x_new_pt[i_pt] = s\n",
    "            quality_pt[i_pt] = row[1]\n",
    "            y_pt[i_pt] = row[3]\n",
    "            i_pt += 1\n",
    "    return (x_new_sp, quality_sp, y_sp), (x_new_pt, quality_pt, y_pt)\n",
    "\n",
    "\n",
    "def separate_test(ds, x):\n",
    "    count = ds['language'].value_counts()\n",
    "    x_new_sp = [None] * count['spanish']\n",
    "    idx_sp = [None] * count['spanish']\n",
    "    i_sp = 0\n",
    "    \n",
    "    x_new_pt = [None] * count['portuguese']\n",
    "    idx_pt = [None] * count['portuguese']\n",
    "    i_pt = 0\n",
    "    \n",
    "    for row, s in tqdm(zip(ds.values, x), total=len(x)):\n",
    "        if row[2] == 'spanish':\n",
    "            x_new_sp[i_sp] = s\n",
    "            idx_sp[i_sp] = row[0]\n",
    "            i_sp += 1\n",
    "        else:\n",
    "            x_new_pt[i_pt] = s\n",
    "            idx_pt[i_pt] = row[0]\n",
    "            i_pt += 1\n",
    "    return (x_new_sp, idx_sp), (x_new_pt, idx_pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 20000000/20000000 [00:21<00:00, 927348.56it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 246955/246955 [00:00<00:00, 804356.42it/s]\n"
     ]
    }
   ],
   "source": [
    "(x_sp, q_sp, y_sp), (x_pt, q_pt, y_pt) = separate_train(ds, post_proc)\n",
    "(x_test_sp, idx_sp), (x_test_pt, idx_pt) = separate_test(ds_test, post_proc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds\n",
    "del ds_test\n",
    "del post_proc\n",
    "del post_proc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_words(c, m, min_len=0):\n",
    "    b = set()\n",
    "    for k, v in c.items():\n",
    "        if v > m and len(k) > min_len:\n",
    "            b.add(k)\n",
    "    return b\n",
    "\n",
    "\n",
    "def non_empty_post(post, words):\n",
    "    total = 0\n",
    "    for x in post:\n",
    "        for w in x:\n",
    "            if w in words:\n",
    "                total = total + 1\n",
    "                break\n",
    "    return total\n",
    "\n",
    "def how_many_word(x,c=None, mini=2, maxi=100):\n",
    "    print(c)\n",
    "    res = c\n",
    "    if c is None:\n",
    "        res = count_words(x)\n",
    "        c = res\n",
    "    print('Words: {}'.format(len(c)))\n",
    "    for i in range(mini, maxi+1):\n",
    "        w = filter_words(c, i)\n",
    "        print('Repeats: {} Words:{} Posts: {}'.format(i, len(w), non_empty_post(x, w)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 10000000/10000000 [00:52<00:00, 188814.42it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 10000000/10000000 [00:42<00:00, 233417.79it/s]\n"
     ]
    }
   ],
   "source": [
    "c_sp = count_words(x_sp)\n",
    "c_pt = count_words(x_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def map_words(c, min_rep=24, min_len=0):\n",
    "    return {w: i for i, w in enumerate(filter_words(c, min_rep, min_len))}\n",
    "\n",
    "\n",
    "def word_vectors(posts, words):\n",
    "    return [[words[w] + 1 for w in s if w in words] for s in tqdm(posts)]\n",
    "\n",
    "\n",
    "def map_classes(y):\n",
    "    return {k:i for i, k in tqdm(enumerate(set(y)))}\n",
    "\n",
    "\n",
    "def y_create(y, classes):\n",
    "    return np.asarray([classes[c] for c in tqdm(y)])\n",
    "\n",
    "\n",
    "def weights(quality):\n",
    "    return np.asarray([1 if x == 'reliable' else 0.5 for x in tqdm(quality)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish\n",
      "Words\n",
      "X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 10000000/10000000 [02:27<00:00, 67871.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1574it [00:00, 261800.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 10000000/10000000 [00:05<00:00, 1878628.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 10000000/10000000 [00:03<00:00, 3073143.95it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Spanish')\n",
    "print('Words')\n",
    "words_sp = map_words(c_sp, 8, 1)\n",
    "print('X')\n",
    "x_sp = word_vectors(x_sp, words_sp)\n",
    "print('Classes')\n",
    "classes_sp = map_classes(y_sp)\n",
    "print('y')\n",
    "y_sp = y_create(y_sp, classes_sp)\n",
    "print('Weights')\n",
    "w_sp = weights(q_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portuguese\n",
      "Words\n",
      "X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 10000000/10000000 [09:32<00:00, 17469.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1576it [00:00, 225097.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 10000000/10000000 [00:04<00:00, 2372360.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 10000000/10000000 [00:03<00:00, 3017491.40it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Portuguese')\n",
    "print('Words')\n",
    "words_pt = map_words(c_pt, 8, 1)\n",
    "print('X')\n",
    "x_pt = word_vectors(x_pt, words_pt)\n",
    "print('Classes')\n",
    "classes_pt = map_classes(y_pt)\n",
    "print('y')\n",
    "y_pt = y_create(y_pt, classes_pt)\n",
    "print('Weights')\n",
    "w_pt = weights(q_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 124987/124987 [00:01<00:00, 95262.12it/s]\n"
     ]
    }
   ],
   "source": [
    "x_test_sp = word_vectors(x_test_sp, words_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 121968/121968 [00:00<00:00, 169394.55it/s]\n"
     ]
    }
   ],
   "source": [
    "x_test_pt = word_vectors(x_test_pt, words_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "31\n",
      "17\n",
      "17\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(max(map(len, x_sp)))\n",
    "print(max(map(len, x_pt)))\n",
    "print(max(map(len, x_test_sp)))\n",
    "print(max(map(len, x_test_pt)))\n",
    "print(min(map(len, x_sp)))\n",
    "print(min(map(len, x_pt)))\n",
    "print(min(map(len, x_test_sp)))\n",
    "print(min(map(len, x_test_pt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = 'separated_seq'\n",
    "if not os.path.exists(base_dir):\n",
    "    os.mkdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dump({'x_sp': x_sp, 'x_test_sp': x_test_sp}, open(base_dir + os.sep + 'x_sp.json', 'w', encoding='utf-8'))\n",
    "np.savez_compressed(base_dir + os.sep + 'y_w_idx_sp.npz', y_sp=y_sp, w_sp=w_sp, idx_sp=np.asarray(idx_sp))\n",
    "pickle.dump(classes_sp, open(base_dir + os.sep + 'classes_sp.p', 'wb'))\n",
    "pickle.dump(words_sp, open(base_dir + os.sep + 'words_sp.p', 'wb'))\n",
    "\n",
    "json.dump({'x_pt': x_pt, 'x_test_pt': x_test_pt}, open(base_dir + os.sep + 'x_pt.json', 'w', encoding='utf-8'))\n",
    "np.savez_compressed(base_dir + os.sep + 'y_w_idx_pt.npz', y_pt=y_pt, w_pt=w_pt, idx_pt=np.asarray(idx_pt))\n",
    "pickle.dump(classes_pt, open(base_dir + os.sep + 'classes_pt.p', 'wb'))\n",
    "pickle.dump(words_pt, open(base_dir + os.sep + 'words_pt.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
